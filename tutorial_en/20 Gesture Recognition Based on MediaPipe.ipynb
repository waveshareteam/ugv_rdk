{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6282ee6c-0e57-40da-806c-86dfa039ea1f",
   "metadata": {},
   "source": [
    "# Gesture Recognition Based on MediaPipe\n",
    "\n",
    "This section introduces how to implement gesture recognition using MediaPipe + OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3205ce-5a8f-4d79-b4b1-a5a652d1646b",
   "metadata": {},
   "source": [
    "## What is MediaPipe?\r\n",
    "\n",
    "MediaPipe is an open-source framework developed by Google for building machine learning-based multimedia processing applications. It provides a set of tools and libraries for processing video, audio, and image data, and applies machine learning models to achieve various functionalities such as pose estimation, gesture recognition, and face detection. MediaPipe is designed to offer efficient, flexible, and easy-to-use solutions, enabling developers to quickly build a variety of multimedia processing applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb510e4a-d332-4c27-81f6-59c7c85cd170",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Since the product automatically runs the main program at startup, which occupies the camera resource, this tutorial cannot be used in such situations. You need to terminate the main program or disable its automatic startup before restarting the robot.\n",
    "\n",
    "It's worth noting that because the robot's main program uses multi-threading and is configured to run automatically at startup through crontab, the usual method sudo killall python typically doesn't work. Therefore, we'll introduce the method of disabling the automatic startup of the main program here.\n",
    "\n",
    "### Terminate the Main Program\n",
    "\n",
    "1. Click the \"+\" icon next to the tab for this page to open a new tab called \"Launcher.\"\n",
    "2. Click on \"Terminal\" under \"Other\" to open a terminal window.\n",
    "3. Type bash into the terminal window and press Enter.\n",
    "4. Now you can use the Bash Shell to control the robot.\n",
    "5. Enter the command: `sudo killall -9 python`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c9488-47b8-4bec-8e2b-ccba2486f55d",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The following code block can be run directly:\n",
    "\n",
    "1. Select the code block below.\n",
    "2. Press Shift + Enter to run the code block.\n",
    "3. Watch the real-time video window.\n",
    "4. Press `STOP` to close the real-time video and release the camera resources.\n",
    "\n",
    "### If you cannot see the real-time camera feed when running:\n",
    "\n",
    "- Click on Kernel -> Shut down all kernels above.\n",
    "- Close the current section tab and open it again.\n",
    "- Click `STOP` to release the camera resources, then run the code block again.\n",
    "- Reboot the device.\n",
    "\n",
    "### Features of this Section\n",
    "\n",
    "When the code block runs successfully, you can place your hand in front of the camera, and the real-time video frame will display annotations indicating the joints of the hand. These annotations will change with the movement of your hand, and the positions of each joint will be outputted as well, facilitating further development for gesture control.\r\n",
    "\r\n",
    "MediaPipe's gesture recognition process uses different names to correspond to different joints. You can retrieve the position information of a joint by calling its corresponding number.\n",
    "\n",
    "#### MediaPipe Han\n",
    "d\n",
    "1.WRIST\n",
    "\n",
    "2.THUMB_CMC\n",
    "\n",
    "3.THUMB_MCP\n",
    "\n",
    "4.THUMB_IP\n",
    "\n",
    "5.THUMB_TIP\n",
    "\n",
    "6.INDEX_FINGER_MCP\n",
    "\n",
    "7.INDEX_FINGER_PIP\n",
    "\n",
    "8.INDEX_FINGER_DIP\n",
    "\n",
    "9.INDEX_FINGER_TIP\n",
    "\n",
    "10.MIDDLE_FINGER_MCP\n",
    "\n",
    "11.MIDDLE_FINGER_PIP\n",
    "\n",
    "12.MIDDLE_FINGER_DIP\n",
    "\n",
    "13.MIDDLE_FINGER_TIP\n",
    "\n",
    "14.RING_FINGER_MCP\n",
    "\n",
    "15.RING_FINGER_PIP\n",
    "\n",
    "16.RING_FINGER_DIP\n",
    "\n",
    "17.RING_FINGER_TIP\n",
    "\n",
    "18.PINKY_MCP\n",
    "\n",
    "19.PINKY_PIP\n",
    "\n",
    "20.PINKY_DIP\n",
    "\n",
    "21.PINKY_TIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30eb25-3807-4b5c-9702-13f393c32fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # Import the OpenCV library for image processing\n",
    "import imutils, math  # Auxiliary libraries for image processing and mathematical operations\n",
    "from IPython.display import display, Image  # Library for displaying images in Jupyter Notebook\n",
    "import ipywidgets as widgets  # Library for creating interactive widgets such as buttons\n",
    "import threading  # Library for creating new threads to execute tasks asynchronously\n",
    "import mediapipe as mp  # Import the MediaPipe library for hand keypoint detection\n",
    "\n",
    "# Create a \"Stop\" button that allows the user to stop the video stream by clicking on it\n",
    "# ================\n",
    "stopButton = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Stop',\n",
    "    disabled=False,\n",
    "    button_style='danger',  # Button style: 'success', 'info', 'warning', 'danger', or ''\n",
    "    tooltip='Description',\n",
    "    icon='square'  # FontAwesome icon name (without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "# Initialize MediaPipe drawing utilities and hand keypoint detection model\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(max_num_hands=1)  # Initialize the hand keypoint detection model to detect up to one hand\n",
    "\n",
    "# Define the display function to process video frames and perform hand keypoint detection\n",
    "def view(button):\n",
    "    camera = cv2.VideoCapture(-1) \n",
    "    camera.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    \n",
    "    display_handle=display(None, display_id=True)  # Create a display handle to update the displayed image\n",
    "    \n",
    "    while True:\n",
    "        # frame = picam2.capture_array()\n",
    "        _, frame = camera.read()\n",
    "        # frame = cv2.flip(frame, 1) # If your camera reverses your image\n",
    "\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        results = hands.process(img)\n",
    "\n",
    "        # If hand keypoints are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for handLms in results.multi_hand_landmarks:  # Iterate through each detected hand\n",
    "                # Draw hand keypoints\n",
    "                for id, lm in enumerate(handLms.landmark):\n",
    "                    h, w, c = img.shape\n",
    "                    cx, cy = int(lm.x * w), int(lm.y * h)  # Calculate the position of the keypoint in the image\n",
    "                    cv2.circle(img, (cx, cy), 5, (255, 0, 0), -1)  # Draw a circle at the keypoint position\n",
    "\n",
    "                \n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                mpDraw.draw_landmarks(frame, handLms, mpHands.HAND_CONNECTIONS)  # Draw hand skeleton connections\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) \n",
    "\n",
    "                target_pos = handLms.landmark[mpHands.HandLandmark.INDEX_FINGER_TIP]\n",
    "\n",
    "        _, frame = cv2.imencode('.jpeg', frame)\n",
    "        display_handle.update(Image(data=frame.tobytes()))\n",
    "        if stopButton.value==True:\n",
    "            # picam2.close() # If yes, close the camera\n",
    "            cv2.release() # If yes, close the camera\n",
    "            display_handle.update(None)\n",
    "\n",
    "# Display the \"Stop\" button and start a thread to execute the display function\n",
    "# ================\n",
    "display(stopButton)\n",
    "thread = threading.Thread(target=view, args=(stopButton,))\n",
    "thread.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
